# -*- coding: utf-8 -*-
"""Web_app

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19qfa402xLUhgKkm6V1B3jMJbMZCsCCuz
"""

!pip install streamlit gensim xgboost scikit-learn nltk joblib

import re
import nltk
import gensim
import joblib
import numpy as np
import streamlit as st
from nltk.tokenize import word_tokenize
from gensim.models import Word2Vec
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.preprocessing import StandardScaler

# Download necessary NLTK resources
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# Initialize objects
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

# Load pre-trained Word2Vec model
word2vec_model = joblib.load('word2vec_model.joblib')

# Load pre-trained XGBoost model
model = joblib.load('best_model.joblib')

# Load LabelEncoder
encoder = joblib.load('label_encoder.joblib')
class_names = encoder.classes_

# Load pre-fitted StandardScaler
scaler = joblib.load('scaler.joblib')

# Function to clean text
def clean_text(text):
    if not isinstance(text, str):
        return ""
    text = text.lower()                  # Convert to lowercase
    text = re.sub(r'http\S+', '', text)  # Remove URLs
    text = re.sub(r'@\w+', '', text)     # Remove mentions
    text = re.sub(r'#\w+', '', text)     # Remove hashtags
    text = re.sub(r'\d+', '', text)      # Remove numbers
    text = re.sub(r'[^\w\s]', '', text)  # Remove punctuation
    tokens = word_tokenize(text)         # Tokenize
    cleaned_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]
    return " ".join(cleaned_tokens)      # Return cleaned text as a string

# Function to compute average Word2Vec embeddings for a text
def get_avg_word2vec(tokens, model, vector_size):
    valid_tokens = [token for token in tokens if token in model.wv.key_to_index]
    if not valid_tokens:
        return np.zeros(vector_size)
    return np.mean([model.wv[token] for token in valid_tokens], axis = 0)

# Streamlit UI components
st.title('Binary Review Classification')
st.write('Enter a review, and the model will classify it into its corresponding label.')

review_text = st.text_area('Enter the review here:')

# When the user submits a review
if review_text:
    # Clean the text
    cleaned_review = clean_text(review_text)
    st.write(f"**Cleaned Review:** {cleaned_review}")

    # Tokenize the cleaned review
    tokens = cleaned_review.split()

    # Convert the review to its Word2Vec embedding
    review_embedding = get_avg_word2vec(tokens, word2vec_model, 100)

    # Rescale the embedding (use the pre-fitted scaler)
    review_embedding_scaled = scaler.transform([review_embedding])

    # Predict using the pre-trained XGBoost model
    prediction = model.predict(review_embedding_scaled)
    predicted_label = encoder.inverse_transform(prediction)[0]

    # Display the prediction result
    st.write(f"The predicted label is: **{predicted_label}**")

    # Optional: Display class mapping for reference
    st.write("Class Mapping:")
    st.write({i: label for i, label in enumerate(class_names)})